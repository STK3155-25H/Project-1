% File: ols_mse.tex
\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
    width=8cm,
    height=6cm,
    xlabel={Polynomial degree},
    ylabel={MSE},
    title={LASSO Regression With Advanced SGD Methods},
    legend pos=north east,
    grid=both,
    y tick label style={
        /pgf/number format/fixed,
        /pgf/number format/precision=2
    },
    ]
    \addplot [blue, mark=none] table [x=degree, col sep=comma] {../outputs/tables/part_f_lasso_mse_momentum_lr=0.01_sgd.csv};
    \addlegendentry{$momentum$}

    \addplot [orange, mark=none] table [x=degree, col sep=comma] {../outputs/tables/part_f_lasso_mse_adagrad_lr=0.01_sgd.csv};
    \addlegendentry{$adagrad$}

    \addplot [green!60!black, mark=none] table [x=degree, col sep=comma] {../outputs/tables/part_f_lasso_mse_rmsprop_lr=0.01_sgd.csv};
    \addlegendentry{$rmsprop$}

    \addplot [red, mark=none] table [x=degree, col sep=comma] {../outputs/tables/part_f_lasso_mse_adam_lr=0.01_sgd.csv};
    \addlegendentry{$adam$}
\end{axis}
\end{tikzpicture}
\caption{Mean squared error (MSE) as a function of polynomial degree for Lasso regression using different stochastic gradient descent methods, with learning rate $0.01$ and regularization parameter $\lambda=0.01$. The plot compares the performance of momentum, Adagrad, RMSprop, and Adam optimizers, highlighting differences in convergence and stability across polynomial degrees.}
\label{fig:AGD_LASSO_SGD}
\end{figure}
