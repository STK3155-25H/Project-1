% File: ols_mse.tex
\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
    width=8cm,
    height=6cm,
    xlabel={Polynomial degree},
    ylabel={MSE},
    title={LASSO Regression With Advanced GD Methods},
    legend pos=north east,
    grid=both,
    y tick label style={
        /pgf/number format/fixed,
        /pgf/number format/precision=2
    },
    ]
    \addplot [blue, mark=none] table [x=degree, col sep=comma] {../outputs/tables/part_e_lasso_mse_vanilla_lr=0.01.csv};
    \addlegendentry{$vanilla$}

    \addplot [orange, mark=none] table [x=degree, col sep=comma] {../outputs/tables/part_e_lasso_mse_momentum_lr=0.01.csv};
    \addlegendentry{$momentum$}

    \addplot [green!60!black, mark=none] table [x=degree, col sep=comma] {../outputs/tables/part_e_lasso_mse_adagrad_lr=0.01.csv};
    \addlegendentry{$adagrad$}

    \addplot [red, mark=none] table [x=degree, col sep=comma] {../outputs/tables/part_e_lasso_mse_rmsprop_lr=0.01.csv};
    \addlegendentry{$rmsprop$}

    \addplot [purple, mark=none] table [x=degree, col sep=comma] {../outputs/tables/part_e_lasso_mse_adam_lr=0.01.csv};
    \addlegendentry{$adam$}
\end{axis}
\end{tikzpicture}
\caption{Test MSE of LASSO regression across polynomial degrees for different optimization methods. Comparison of vanilla gradient descent, momentum, Adagrad, RMSProp, and Adam with learning rate $0.01$ and regularization parameter $\lambda=0.01$, showing how model complexity affects generalization performance.}
\label{fig:AGD_LASSO}
\end{figure}
