 \documentclass[
 reprint,            % two-column layout
 amsmath,amssymb,
 aps,
]{revtex4-2}



% Packages
% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{chngcntr}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{caption}
\captionsetup{hypcap=true}
\usepackage{algorithm}
\usepackage{algpseudocode}


\pgfplotsset{compat=1.18}

\begin{document}

\title{Analyzing Different Regression and Resampling Methods}

\author{Stan Daniels}
\author{Francesco Minisini}
\author{Teresa Ghirlandi}
\author{Carolina Ceccacci}
\affiliation{University of Oslo \\ Data Analysis and Machine Learning (FYS-STK3155/FYS4155)}

\date{October 6, 2025}

\begin{abstract}
Regression in machine learning is a fundamental technique for predicting outcomes based on input features. It finds relationships between variables so that predictions on unseen data can be made. 
A major challenge arises as model complexity increases:  low-degree models may underfit, while high-degree polynomial regression can become unstable and overfit the data. 
In this project, we study the Runge function, a well-known function that highlights the difficulties of high-degree polynomial interpolation. We apply Ordinary Least Squares, Ridge, and Lasso regression, complemented by gradient descent and its variants, including momentum, Adagrad, RMSprop, and ADAM. Resampling techniques such as bootstrap and cross-validation are used to evaluate model generalization and analyze the bias-variance trade-off.
The results show that OLS fits become highly unstable for high polynomial degrees, while Ridge and Lasso regularization significantly improve stability and predictive accuracy. Gradient descent methods reproduce the analytical results, though their performance depends strongly on learning-rate strategies. Overall, the study highlights the importance of regularization and resampling for controlling overfitting and improving the reliability of regression models.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:Introduction}
Regression is a cornerstone in data analysis, machine learning, and scientific modeling, enabling the approximation of relationships between variables and facilitating predictions.
As datasets become larger and larger, choosing the right regression method is crucial, as it directly affects model accuracy, stability, and interpretability\cite{mehta2019highbias}.
Understanding the performance of different regression techniques is therefore a central problem in statistical learning and computational physics \cite{aiken2021framework}.
In this project, various regression methods are investigated, such as Ordinary Least Squares, Ridge Regression, and Lasso Regression.
It focuses on fitting polynomials to a specific one-dimensional function, the Runge function:
\[
\frac{1}{1+25x^2}, \quad x\in[-1, 1]
\]
The Runge function serves as a classical example that illustrates the challenges of high-degree polynomial interpolation, such as oscillations near the boundaries, known as Runge's phenomenon\cite{runge1901}. This makes it an ideal test case to compare the performances of the different methods.
First, an OLS regression analysis is performed, exploring the dependence on the number of data points and the degree of polynomial. The analysis is then extended to Ridge and Lasso regressions, which add a regularization parameter ${\lambda}$. Gradient descent methods are implemented.
The analysis starts with the standard gradient descent method, but then, to improve efficiency and convergence, several variants of the gradient descent have been developed, such as momentum, stochastic gradient descent and adaptive methods, including Adagrad, RMSprop, ADAM.
The performance of OLS, Ridge and Lasso is then compared with the gradient descent-based optimization methods.
In order to evaluate model performance and investigate bias-variance trade-off, resampling techniques such as bootstrap and cross-validation are applied, highlighting how different choices of model complexity and regularization affect the trade-off between bias and variance.
These techniques provide insight into the stability of the models and the reliability of their predictions. 
Overall, this project aims to illustrate the strengths and the limitations of each method.\\
The structure of this project is as follows: 
\begin{itemize}
    \item Section \ref{sec:Methods} "Methods", describes the preprocessing and scaling of the data, the regression techniques and the optimization algorithms, as well as the resampling methods. It also includes the testing of the implemented methods against scikit-learn and the AI tools used.
    \item Section \ref{sec:Results_and_Discussion} "Results and Discussion", presents the numerical results, compares the performance of the different methods and discusses their implications in terms of bias-variance trade-off.
    \item section \ref{sec:Discussion_and_Conclusion} "Conclusion", summarizes the main results and the insights gained from the methods studied.
\end{itemize}


\section{Methods}
\label{sec:Methods}
 Let $\mathbf{y} \in \mathbb{R}^n$ denote the vector of target values and $\mathbf{X} \in \mathbb{R}^{n \times p}$ the design matrix containing $p$ predictors for $n$ observations. The following linear model is assumed:
$$
\mathbf{y} = \tilde{\mathbf{y}} + \boldsymbol{\epsilon}, \quad \text{with} \quad \tilde{\mathbf{y}} = \mathbf{X}\boldsymbol{\theta},
$$

where $\tilde{\mathbf{y}}$ represents the predictions of the model, $\boldsymbol{\theta} \in \mathbb{R}^p$ is the vector of unknown coefficients to be estimated and $\boldsymbol{\epsilon}$ is a vector of errors, typically assumed to be independent and identically distributed with zero mean and variance $\sigma^2$.

The goal of regression is to find an estimate of the optimal parameter $\boldsymbol{\theta}$ that best explains the observed data according to a chosen criterion. 

A detailed description of the methods follows.

\subsection{Scaling Of The Data}
Before applying any regression and optimization methods, the dataset was split into a training and testing dataset and scaled appropriately.
This was performed using the \texttt{split\_scale} and \texttt{polynomial\_features\_scaled} functions.\\
First the training and test sets were seperated.
The input feature $x$ was then standardized using statistics computed on the training set: each feature column was centered by subtracting its mean and scaled by dividing by its standard deviation.
This process produces features with zero mean and unit variance \cite{hjorthjensen_week35}.
The same mean and standard deviation from the training set were applied to scale the test set, avoiding any data leakage.\\
For polynomial regression, each feature was expanded into polynomial terms up to a maximum degree.
After generating the polynomial features, each column (except the intercept) was scaled using the mean and standard deviation of the corresponding column from the training set, via the \texttt{polynomial\_features\_scaled} function.
This ensures that all polynomial features are on a comparable scale across training and test sets.\\\\
Scaling was necessary for several reasons.\\
First, high-degree polynomial terms can grow very large, causing numerical instability in the design matrix.
Second, regularization methods like Ridge and Lasso assume that all features are on a comparable scale, otherwise the penalty terms distort model behavior.
Third, gradient-based optimizers converge faster and more stably when the features are standardized.
Finally, applying training-set statistics to scale the test set ensures unbiased evaluation.


\subsection{Ordinary Least Squares}
Ordinary Least Squares (OLS) is the classical method for linear regression and it estimates $\boldsymbol{\theta}$ by minimizing the mean squared error\cite{hjorthjensen_week35}. This is the cost function that is going to be optimize:

$$
C(\boldsymbol{\theta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}$$

It means that it's required that the derivative with respect to $\boldsymbol{\theta}$ be set equal to zero: $$\frac{\partial C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta_j}} 
= 0 = \mathbf{X}^\top \big(\mathbf{y} - \mathbf{X}\boldsymbol{\theta}\big)
$$

$$\mathbf{X}^\top \mathbf{y}= \mathbf{X}^\top \mathbf{X} \boldsymbol{\theta}$$

For a full-rank design matrix, this has the closed-form solution:
$$
\hat{\boldsymbol{\theta}}_{\text{OLS}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$

OLS provides a simple solution, suitable when the features are few and not highly correlated.

\subsubsection{Implementation}
The inputs of the OLS function are a feature matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$ and a vector of targets $\mathbf{y} \in \mathbb{R}^n$. The output is the vector of the parameters $\boldsymbol{\theta} \in \mathbb{R}^{p}$, whose fomula is given above.\\
The function is implemented in Python, using the NumPy library for efficient numerical computations. The function \texttt{np.linalg.pinv} is used to compute the pseudoinverse of the matrix $\mathbf{X}^\top \mathbf{X}$, which is particularly useful when $\mathbf{X}^\top \mathbf{X}$ is not invertible. This ensures numerical stability.

\subsection{Ridge regression}
A regularization parameter $\lambda$ can be introduced by defining a new cost function to be optimized\cite{hjorthjensen_week35}, that is:
$$ C(\boldsymbol{\theta}) 
=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\theta}\vert\vert_2^2
$$
where the second term represents an $L_2$ penalty on the size of the coefficients.
This leads to the Ridge regression minimization problem where it is required that $ |\boldsymbol{\theta}\|_2^2 \leq t$, where $t$ is a finite positive number. 
One of the main motivations behind Ridge is its ability to resolve the problem of non-invertibility of $\mathbf{X}^\top \mathbf{X}$, which often arises when features are highly correlated. Ridge regression resolves this problem by adding the parameter $\boldsymbol{\lambda}$ to the diagonal of $\mathbf{X}^\top \mathbf{X}$ before inverting it.
Taking the derivatives with respect to $\boldsymbol{\theta}$ the optimal parameters are obtained:
$$ \boldsymbol{\hat\theta_{Ridge}}=(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})^{-1} \mathbf{X}^\top \mathbf{y}$$
with $\mathbf{I}$ being a $p\times p$ identity matrix.

\subsubsection{Implementation}
The inputs of the Ridge function are a feature matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$, a vector of targets $\mathbf{y} \in \mathbb{R}^n$, the regularization parameter $\lambda$ and the intercept. The output is the vector of the parameters $\boldsymbol{\theta} \in \mathbb{R}^{p}$, whose fomula is given above.\\
The function uses the same function \texttt{np.linalg.pinv} to compute the pseudoinverse of the matrix $\mathbf{X}^\top \mathbf{X} + \boldsymbol{\lambda}\mathbf{I}$, ensuring numerical stability.

\subsection{Lasso regression}
Here the regularization term is based on the $L_1$ norm of the parameters\cite{hjorthjensen_week35}.
The cost function is defined as
$$
C(\boldsymbol{\theta}) 
= \frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\vert\vert_2^2
+ \lambda \, \|\boldsymbol{\theta}\|_1,
$$
where $\|\boldsymbol{\theta}\|_1 = \sum_{j}|\theta_j|$.  
This formulation leads to the Lasso minimization problem where it is required that $\|\boldsymbol{\theta}\|_1 \leq t$, with $t$ being a finite positive number. 

Unlike Ridge regression, taking the derivatives with respect to $\boldsymbol{\theta}$ does not lead to an analytical solution. 
The optimization problem can however be solved by using iterative gradient descent methods.\\ \\
The key feature of Lasso lies in its ability to shrink some estimated coefficients $\hat{\theta}_j$ exactly to zero. 
When this happens, the corresponding predictor is completely removed from the model.  
In contrast, Ridge regression never eliminates variables: it only shrinks the coefficients $\hat{\theta}_j$ towards zero but keeps all predictors in the model.
Typically, Lasso Regression is preferred when the goal is to simplify the model and improve interpretability, especially when there are a lot of features. On the other hand, Ridge regression is better for handling multicollinearity among features. 

\subsubsection{Implementation}
? 

\subsection{Gradient descent}

Although OLS and Ridge regression have analytical solutions, such solutions are not always available in general, so a numerical approach is often needed to optimize the same cost function.\\

Consider the cost function $C(\boldsymbol{\theta})$ that has to be minimized with respect to the parameters $\boldsymbol{\theta}$\cite{hjorthjensen_week36}.  
A second-order Taylor expansion around a point $\boldsymbol{\theta}_n$ is performed:
$$
C(\boldsymbol{\theta}) \approx C(\boldsymbol{\theta}_n) + (\boldsymbol{\theta}-\boldsymbol{\theta}_n)^\top \nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}_n) 
+ \frac{1}{2} (\boldsymbol{\theta}-\boldsymbol{\theta}_n)^\top \mathbf{H}(\boldsymbol{\theta}_n) (\boldsymbol{\theta}-\boldsymbol{\theta}_n),
$$

where $\mathbf{H}(\boldsymbol{\theta}_n)$ is the Hessian matrix of second derivatives at $\boldsymbol{\theta}_n$.

Neglecting the second-order term (or assuming it is costly to compute), a first-order approximation gives the update rule in the direction of the steepest descent:
\[
\boldsymbol{\theta}_{n+1} = \boldsymbol{\theta}_n - \eta \, \nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}_n),
\]
where $\eta > 0$ is a learning rate controlling the step size.  

This iterative procedure moves the parameters towards the minimum of $C(\boldsymbol{\theta})$. For convex functions such as the mean squared error in linear regression, convergence is guaranteed if $\eta$ is chosen appropriately.

\subsubsection{Implementation}
The different gradient descent methods were implemented inside the general routine \verb|Gradient_descent_advanced|, in which a specific method can be selected using, for example, \verb|method='rmsprop'| for Rmsprop and so on. \\
In the vanilla gradient descent, the iteration start from an initial guess $\boldsymbol{\theta}^{(0)}$, the parameters are updated iteratively according to the rule:
\[
\boldsymbol{\theta}^{(n+1)} = \boldsymbol{\theta}^{(n)} - \eta \, \nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}^{(n)}),
\]
where $\eta > 0$ is the learning rate and $\nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}^{(n)})$ is the gradient of the cost function with respect to $\boldsymbol{\theta}$ at iteration $t$.
After each update, the algorithm recomputes the cost function and its gradient until a convergence criterion is met.\\

\begin{algorithm}[H]
\caption{Gradient Descent}
\begin{algorithmic}[1]
\State Initialize $\boldsymbol{\theta}^{(0)}$
\For{$n = 0, 1, 2, \dots$ until convergence}
    \State Compute gradient $\nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}^{(n)})$
    \State Update parameters: 
    $\boldsymbol{\theta}^{(n+1)} = \boldsymbol{\theta}^{(n)} - \eta \, \nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}^{(n)})$
\EndFor
\end{algorithmic}
\end{algorithm}

(AAA: I don't know if we want something like this)

\subsection{Stochastic Gradient Descent (SGD)}
Gradient descent can be computationally expensive when applied to large datasets.
Stochastic Gradient Descent (SGD) addresses this by updating the model parameters using a single (or small batch of) data point(s) per iteration, rather than the full dataset\cite{hjorthjensen_week37}.
This introduces noise in the updates but significantly reduces computation time.
The update rule is as follows:  
$$
\boldsymbol{\theta}_{n+1} = \boldsymbol{\theta}_n - {\eta \boldsymbol{\nabla}} l_{i_n} (\boldsymbol{\theta_t}),
$$  
where \(i_t\) is a randomly selected data point and \(\eta\) is the learning rate.\\

While Stochastic Gradient Descent is faster and more memory-efficient than full-batch gradient descent, it introduces noise in the parameter updates, which can make convergence less stable and less precise. However, this stochasticity can be beneficial, as it can help escape poor local minima and explore the optimization landscape more effectively, making it well suited for large datasets. \cite{hjorthjensen_week37}

\subsubsection{Implementation}

In the implementation of Stochastic Gradient Descent (SGD), the parameter vector $\boldsymbol{\theta}$ is initialized to zero and updated iteratively using gradients computed on randomly selected data points or mini-batches, rather than the entire dataset. This reduces the computational cost per update and introduces stochasticity into the optimization trajectory, which can help escape shallow local minima. The batch size is a tunable parameter: when set to 1, the algorithm uses a single sample at each iteration; larger batch sizes trade off between noise reduction and computational efficiency.  
\begin{algorithm}[H]
\caption{Stochastic Gradient Descent}
\begin{algorithmic}[1]
\State Initialize $\boldsymbol{\theta}^{(0)}$
\For{$n = 0, 1, 2, \dots$ until convergence}
    \State Randomly select a data point or mini-batch $i_t$
    \State Compute stochastic gradient: $\mathbf{g} \gets \nabla_{\boldsymbol{\theta}} \ell_{i_t}(\boldsymbol{\theta}^{(n)})$
    \State Update parameters: $\boldsymbol{\theta}^{(n+1)} \gets \boldsymbol{\theta}^{(n)} - \eta \, \mathbf{g}$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Momentum}

The learning rate $\eta$ plays a crucial role in the convergence and a limitation of these methods is the fixed learning rate ${\eta}$: 
\begin{itemize}
    \item if $\eta$ is too large, the updates can overshoot the minimum, causing oscillations or divergence
    \item if $\eta$ is too small, convergence is very slow
\end{itemize}
Moreover, for a function with steep directions and flat directions, a single global $ \eta$ may be inappropriate:
steep coordinates require a smaller step size to avoid oscillation and flat coordinates could use a larger step to speed up progress.

In order to mitigate this problem, gradient descent with momentum is introduced: it refers to a method that smoothens the optimization trajectory by adding a term that helps the optimizer remember the past gradients\cite{hjorthjensen_week37}.

Mathematically, let $\boldsymbol{m}_n$ denote the velocity (or accumulated gradient) at iteration $n$. The update rules for gradient descent with momentum are:
\[
\boldsymbol{m}_{n+1} = \beta \, \boldsymbol{m}_n + (1-\beta)\nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}_n),
\]
\[
\boldsymbol{\theta}_{n+1} = \boldsymbol{\theta}_n - \eta \, \boldsymbol{m}_{n+1}.
\]
where $\beta \in [0,1)$ is the momentum coefficient, which controls how much of the past gradients are remembered in the current update. A value close to 1 means the optimizer will have more inertia while a value closer to 0 means less reliance on past gradients. This mechanism enables the algorithm to suppress oscillations along steep directions while simultaneously accelerating progress across flatter regions of the cost surface. The result is a convergence process that is both faster and more stable than standard gradient descent.\\
This leads to more advanced optimization methods which use an adaptive learning rate for each parameter that depends on the history of gradients.\cite{hjorthjensen_week37}\\

\subsubsection{Implementation}

In addition to the parameter vector $\boldsymbol{\theta}$, a velocity vector $\mathbf{m}$ of the same size is initialized to zero. At each iteration the velocity is updated as a moving average of past gradients, scaled by the momentum coefficient $\beta$, and the parameters are updated using this velocity.
The default value $\beta=0.9$ was used in the experiments, providing a balance between stability and acceleration. Convergence is checked by comparing the change in parameters with a tolerance threshold.
\begin{algorithm}[H]
\caption{Gradient Descent with Momentum}
\begin{algorithmic}[1]
\State Initialize $\boldsymbol{\theta}^{(0)}$, velocity $\mathbf{m}^{(0)}=0$
\For{$n = 0, 1, 2, \dots$ until convergence}
\State Compute gradient $\mathbf{g} \gets \nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}^{(n)})$
\State Update velocity: $\mathbf{m} \gets \beta \mathbf{m} + (1-\beta)\mathbf{g}$
\State Update parameters: $\boldsymbol{\theta}^{(n+1)} \gets \boldsymbol{\theta}^{(n)} - \eta \mathbf{m}$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Adagrad}

Adagrad adapts the learning rate for each parameter based on the historical squared gradients, giving smaller updates to frequently updated parameters and larger updates to infrequent ones\cite{hjorthjensen_week37}.
This is particularly useful for sparse data.  
\[
\boldsymbol{\theta}_{n+1} = \boldsymbol{\theta}_n - \frac{\eta}{\sqrt{\boldsymbol{v}_n} + \epsilon} \, \nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}_n).
\]

where \(v_n\) is the sum of squared gradients up to time \(n\) and \(\epsilon\) prevents division by zero.  \\

The accumulation of squared gradients can lead to excessively small learning rates over time, slowing down convergence. \cite{goodfellow2016}

\subsubsection{Implementation}

An accumulator vector $\mathbf{G}$ of the same size as $\boldsymbol{\theta}$ is initialized to zero. At each iteration, the squared gradient is added to $\mathbf{G}$ and used to scale the learning rate for each parameter.
To ensure numerical stability, a small constant $\epsilon=10^{-8}$ is added before taking the square root. This prevents division by zero and is standard in Adagrad implementations.
\begin{algorithm}[H]
\caption{Adagrad}
\begin{algorithmic}[1]
\State Initialize $\boldsymbol{\theta}^{(0)}$, accumulator $\mathbf{G}^{(0)}=0$
\For{$n = 0, 1, 2, \dots$ until convergence}
\State Compute gradient $\mathbf{g} \gets \nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}^{(n)})$
\State Update accumulator: $\mathbf{G} \gets \mathbf{G} + \mathbf{g}^2$
\State Update parameters: $\boldsymbol{\theta}^{(n+1)} \gets \boldsymbol{\theta}^{(n)} - \eta \, \mathbf{g} / (\sqrt{\mathbf{G}}+\epsilon)$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{RMSprop}

RMSprop tackles Adagrad's excessive shrinkage of the learning rate by using an exponentially decaying average of past squared gradients instead of a simple sum\cite{hjorthjensen_week37}.
This prevents the learning rate from shrinking too much and slowing down.  
If the running average is
\[
v_n = \beta v_{n-1} + (1 - \beta)\left( \nabla C(\theta_n) \right)^2,
\]
with decay rate \(\beta\) 0.9 or 0.99,

the new parameter update is
\[\theta_{n+1} = \theta_n - \frac{\eta}{\sqrt{v_n + \epsilon}} \nabla C(\theta_n).
\]

On the other hand, RMSprop requires tuning of the decay rate \(\beta\) and the base learning rate \(\eta\). \cite{goodfellow2016}

\subsubsection{Implementation}

Instead of storing the full sum of squared gradients, an exponentially decaying moving average is maintained in a vector $\mathbf{S}$ initialized to zero. At each iteration, $\mathbf{S}$ is updated using the decay rate $\rho$ (set to 0.9 in our experiments), and the gradient is rescaled accordingly.
As with Adagrad, a small constant $\epsilon=10^{-8}$ is added for stability. This prevents excessively small denominators when gradients are close to zero.
\begin{algorithm}[H]
\caption{RMSprop}
\begin{algorithmic}[1]
\State Initialize $\boldsymbol{\theta}^{(0)}$, accumulator $\mathbf{S}^{(0)}=0$
\For{$n = 0, 1, 2, \dots$ until convergence}
\State Compute gradient $\mathbf{g} \gets \nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}^{(n)})$
\State Update moving average: $\mathbf{S} \gets \rho \mathbf{S} + (1-\rho)\mathbf{g}^2$
\State Update parameters: $\boldsymbol{\theta}^{(n+1)} \gets \boldsymbol{\theta}^{(n)} - \eta , \mathbf{g} / (\sqrt{\mathbf{S}}+\epsilon)$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Adam}

Adam (Adaptive Moment Estimation) combines momentum and RMSprop, maintaining both an exponentially decaying average of past gradients (first moment) and squared gradients (second moment), with bias correction\cite{hjorthjensen_week37}. It adapts the learning rate per parameter, providing stability, while incorporating momentum, accelerating the convergence.
So, combining the moving averages of momentum
\[
m_n = \beta_1 m_{n-1} + (1-\beta_1) \nabla_\theta C(\theta_n)
\]  
and of the squared gradients

\[
v_n = \beta_2 v_{n-1} + (1-\beta_2) (\nabla_\theta C(\theta_n))^2,
\]  
and correcting bias
\[
\hat{m}_n = \frac{m_n}{1-\beta_1^n}, \quad \hat{v}_n = \frac{v_n}{1-\beta_2^n},
\]  
the update rule in Adam becomes
\[
\theta_{n+1} = \theta_n - \frac{\eta}{\sqrt{\hat{v}_n}+\epsilon} \hat{m}_n,
\]  
where \(\epsilon\) prevents division by zero.

Adam can sometimes converge to slightly worse minima than SGD in certain tasks, and hyperparameter tuning is still required. \cite{goodfellow2016}

\subsubsection{Implementation}

Two vectors of the same size as $\boldsymbol{\theta}$ are initialized: the first moment estimate $\mathbf{m}$ and the second moment estimate $\mathbf{v}$, both starting at zero. At each iteration, these estimates are updated with exponential moving averages of the gradient and squared gradient. Bias correction is applied using the current iteration counter $t$.
Default hyperparameters $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=10^{-8}$ were used, consistent with common practice. This implementation therefore combines momentum and RMSprop into a single adaptive optimizer.
\begin{algorithm}[H]
\caption{Adam}
\begin{algorithmic}[1]
\State Initialize $\boldsymbol{\theta}^{(0)}$, $\mathbf{m}^{(0)}=0$, $\mathbf{v}^{(0)}=0$, timestep $t=0$
\For{$n = 0, 1, 2, \dots$ until convergence}
\State $t \gets t+1$
\State Compute gradient $\mathbf{g} \gets \nabla_{\boldsymbol{\theta}} C(\boldsymbol{\theta}^{(n)})$
\State Update moments: $\mathbf{m}, \mathbf{v}$
\State Apply bias correction to $\mathbf{m}, \mathbf{v}$
\State Update parameters: $\boldsymbol{\theta}^{(n+1)}$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Gradient descents comparison}
Stochastic Gradient Descent (SGD) is appreciated for its simplicity and memory efficiency, particularly when tackling large datasets. However, it sometimes oscillates and may converge slowly. 

Momentum builds upon SGD by accelerating convergence in directions of consistent descent, although it can occasionally bypass the lowest point in regions where the cost function curves steeply. 

Adagrad's strength lies in its adaptability to sparse data, but it tends to reduce the learning rate over time, which can stall progress. 

RMSprop was introduced to counter Adagrad's diminishing learning rate, making it effective for online and non-stationary tasks where data characteristics frequently change. 

Adam is more often the preferred method because it merges the advantages of momentum and adaptive learning rates, generally performing robustly in most deep learning scenarios, though it may require careful parameter tuning to reach optimal performance. \cite{goodfellow2016}




\subsection{Bias-variance trade-off and resampling techniques}

The bias-variance trade-off is a fundamental concept in machine learning and statistics that describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model\cite{hjorthjensen_week38}. 
Bias is the error due to approximating a complex real-world problem with a simplified model: 
$$\mathrm{Bias}[\tilde y]= \mathbb{E}[(f-\mathbb{E}[\tilde y])^2]$$

It's the squared difference between the true function f(x) and the average prediction $\mathbb{E}[\tilde y]$ of the model over different training sets. High bias leads to underfitting, where the model is too simple (often linear) and cannot capture the underlying patterns, resulting in large errors on both training and test data.
\\Variance measures the sensitivity of a model to small changes in the training data:
$$
\mathrm{Var}[\tilde y] = \mathbb{E}[\tilde y^2] - (\mathbb{E}[\tilde y])^2
$$
It's the expected squared deviation of the model's predictions from its average prediction.
High variance leads to overfitting, where the model fits the training data very closely but performs poorly on unseen data, showing large test errors.\\\\
More formally, consider a dataset $\{(x_i, y_i)\}_{i=1}^n$, where the true data is generated by a noisy model $y = f(x) + \varepsilon$ and $\varepsilon$ is the noise term with zero mean and variance $\sigma^2$. 
The true function $f(x)$ is approximated by a model $\tilde y$, which depends on parameters $\boldsymbol{\theta}$ and a design matrix $X$, determined by minimizing the mean squared error (MSE):
$$
\text{C}(\boldsymbol{X, \theta}) = \frac{1}{n} \sum_{i=1}^n \big( y_i - \tilde y \big)^2= \mathbb{E}\big[(y - \tilde y)^2\big]
$$

The expected prediction error can be decomposed as
$$\mathbb{E}[(y - \tilde y)^2\big] = \mathrm{Bias}[\tilde y] + \mathrm{Var}[\tilde y] + \sigma^2$$
where $\sigma ^2$ is the variance of the errore $\epsilon$. 

To derive this equation, it should be recalled that the variance of $\boldsymbol{y}$ and $\boldsymbol{\epsilon}$ are both equal to $\sigma^2$. The mean value of $\boldsymbol{\epsilon}$ is by definition equal to zero. Furthermore, the function $f$ is not a stochastics variable, idem for $\boldsymbol{\tilde{y}}$.
Using a more compact notation in terms of the expectation value
$$
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right]=\mathbb{E}\left[(\boldsymbol{f}+\boldsymbol{\epsilon}-\boldsymbol{\tilde{y}})^2\right],
$$

and adding and subtracting $\mathbb{E}\left[\boldsymbol{\tilde{y}}\right]$ one obtains 
$$
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right]=\mathbb{E}\left[(\boldsymbol{f}+\boldsymbol{\epsilon}-\boldsymbol{\tilde{y}}+\mathbb{E}\left[\boldsymbol{\tilde{y}}\right]-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2\right],
$$

which, using the abovementioned expectation values can be rewritten as
$$
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right]=\mathbb{E}\left[(\boldsymbol{y}-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2\right]+\mathrm{Var}\left[\boldsymbol{\tilde{y}}\right]+\sigma^2,
$$

that is the rewriting in terms of the bias, the variance of the model $\boldsymbol{\tilde{y}}$ and the variance of $\boldsymbol{\epsilon}$. \cite{misc} 
In order to derive this equation, the assumption that is made is that the unknown function $\boldsymbol{f}$ can be replaced by the target data $\boldsymbol{y}$.\\
*I'm not able to cite because the .bib is deleting what I'm writing* \\\\ 
This decomposition underlies the bias-variance trade-off: a high bias and low variance situation corresponds to a model that is too simple and underfits the data, while a low bias and high variance situation corresponds to a model that is too complex and overfits the data.
The goal is to find a balance, achieving a model that is complex enough to capture the underlying patterns but simple enough to generalize well to new data.\\\\

Resampling techniques provide the practical tool for measuring and managing this balance, by reliably estimating the generalization ability of the model.
They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.
Two resampling techniques are discussed here: bootstrap and cross-validation.\\\\
Bootstrap is a resampling technique that involves repeatedly drawing samples with replacement from the original dataset to create multiple "bootstrap" samples\cite{hjorthjensen_week38}.
Each bootstrap sample is used to fit the model, and the variability of the model's predictions across these samples provides an estimate of the model's variance.
This method is particularly useful for estimating the uncertainty of model parameters and assessing the stability of the model.\\\\
Cross-validation is a technique used to assess how the results of a statistical analysis will generalize to an independent dataset\cite{hjorthjensen_week38}.
The most common form is k-fold cross-validation, where the dataset is divided into k subsets (or "folds").
The model is trained on k-1 folds and validated on the remaining fold.
This process is repeated k times, with each fold serving as the validation set once.
The average performance across all k trials provides a robust estimate of the model's generalization error.
Cross-validation helps in selecting model parameters and in preventing overfitting by ensuring that the model performs well on unseen data.\\\\

\subsection{Validation and testing}

\subsection{Use of AI tools}
In this project, AI tools such as ChatGPT and GitHub Copilot were utilized to improve the efficiency and quality of the work. 
These tools assisted in generating code snippets, debugging and they were also particularly helpful in drafting sections of the report and suggesting improvements. 
However, all outputs generated by these AI tools were carefully reviewed and modified.


\section{Results and Discussion}
\label{sec:Results_and_Discussion}
The performance of the regression models are evaluated using both the MSE and the score $R^2$. The definitions of these are the following:
$$ \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \tilde{y}_i \right)^2$$
$$R^2 = 1 - \frac{\sum_{i=1}^{n} \left( y_i - \tilde{y}_i \right)^2}{\sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2}$$
where $n$ is the number of data points, $y_i$ are the true values, $\tilde{y}_i$ are the predicted values, and $\bar{y}$ denotes the mean of the true values.
\subsection*{How we produced all reported results (averaging, randomness, and fairness)}
\label{sec:results_setup}
Unless otherwise stated, every result displayed in this section is the mean over $N_{\text{runs}}=30$ independent repetitions. Each run uses a new noisy dataset and a new train/test split, but within a run all models and methods see the same data and splits to enable fair, paired comparisons.

\paragraph*{Data generation and scaling.}
For a given $n$, we create a uniform mesh of Runge function $f(x)=\tfrac{1}{1+25x^2}$ that maps the interval $(-1,1)$. To the mesh we added Gaussian noise \[ y = f(x) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, 0.3^2) \]. We split the data into train/test with a $67\%/33\%$ split and standardize features using training statistics only (the intercept, when present, is not scaled). For polynomial models, features are expanded up to degree $d$ and then scaled column-wise using the training means/stds and re-used on the test set.

\paragraph*{Reproducibility and fairness}
Let $s_0$ be the global base seed. For run index $r=0,\dots,N_{\text{runs}}-1$ we set
\[
s_r = s_0 + r,
\]
as both the NumPy global seed before generating noise and the random\_state for the train/test split. This guarantees that, within the same run, all methods share the exact same dataset and splits. Additional randomness is handled as follows:
\begin{itemize}
    \item \textbf{SGD \ref{Stochastic Gradient Descent}:} to make mini-batch selections identical across models within a run and degree, we reset the RNG to a deterministic ~\cite{numpy-seed}, per-degree seed before each optimizer call (thus the same mini-batch sequence is used by all methods being compared).
    \item \textbf{Bootstrap \ref{Bias-Variance trade off}:} for each run, bootstrap resamples of the training set are drawn with sklearn's resample using fixed per-bootstrap seeds ~\cite{scikit-learn-resample}; predictions are evaluated on the fixed test set of that run.
    \item \textbf{Cross-Validation \ref{Cross Validation}:} for each run we instantiate one KFold with shuffle=True and random\_state $=s_r$ ~\cite{scikit-learn-crossvalidation}, so folds are identical across degrees and models within the run.
\end{itemize}

\paragraph*{What is averaged.}
For parts up to F, each metric displayed at a given degree $d$ is the mean test metric over $N_{\text{runs}}$. For each average metric displayed is also computed its standard deviation. For part G, the MSE, $\mathrm{bias}^2$, and variance are computed from bootstrap predictions within each run and then averaged across runs. For Part~H, per-run degree-$d$ scores are the mean MSE across $k=5$ folds.

% \paragraph*{Modeling/optimization details.}
% OLS and Ridge are also computed in closed form; for Ridge the intercept is not penalized. LASSO is solved with (sub)gradient descent. Gradient-based experiments use:
% \begin{itemize}
%     \item \textbf{Part~C:} vanilla GD with learning rates $\{10^{-4},10^{-3},10^{-2}\}$, $n_{\text{iter}}=2000$, Ridge $\lambda=0.01$.
%     \item \textbf{Part~D:} advanced optimizers (vanilla, momentum, adagrad, rmsprop, adam) at $\eta=0.01$, $n_{\text{iter}}=1000$, for OLS and Ridge ($\lambda=0.01$).
%     \item \textbf{Part~E:} two setups: (i) vanilla with $\eta\in\{10^{-4},10^{-3},10^{-2}\}$; (ii) all methods above at fixed $\eta=0.01$; models: OLS, Ridge ($\lambda=0.01$), LASSO ($\lambda=0.01$).
%     \item \textbf{Part~F (SGD):} same methods at $\eta=0.01$, $n_{\text{iter}}=5000$, mini-batch size $=10$.
% \end{itemize}


% \subsection{Averaging over multiple runs}

\subsection{OLS}
\label{OLS}
The analysis starts with performing a standard ordinary least squares regression using polynomials in $x$ up to order 15. The dependence of the polynomial degree and number of data point has been explored by plotting the mse as a function of polynomial degree for different amount of data points. This can be seen in Figure \ref{fig:OLS_graph}.\\\\
As one can see, generally the mse goes down as the polynomial degree increases. But for a small amount of data points like $n = 40$ and $n = 50$, when the polynomial degree goes to high the mse starts to increase again.\\
This happens because of overfitting. When the polynomial degree increases the model tries to fit the data too perfectly and will fit the underlying random noise as well and the model will then perform poorly of unseen data. For a high number of data points this is barely noticeable but for a lower amount of data points the effect will become more pronounced.\\\\
$R^2$ ?\\\\
When plotting the parameters $\theta$ as a function of polynomial degree, it can be observed that for low-degree polynomials the coefficients remain relatively stable, whereas for higher-degree polynomials they increase rapidly in magnitude and exhibit pronounced oscillations.
This behavior reflects overfitting: at high degrees, the model captures both the true signal and the noise, causing the coefficients to become unstable. Using more data points reduces this effect and stabilizes the parameter estimates
    
\input{Graphs/OLS_MSE.tex}

\subsection{Ridge regression}
\label{Ridge regression}
\textbf{We can also put the theta values graph in and show that the ridge method lowers the extreme high theta values for high degrees.}\\\\
Now, Ridge regression is applied to the same dataset, exploring the impact of different regularization parameters $\lambda$. First, the effect of the regularization parameter $\lambda $ on the MSE as a function of polynomial degree was analyzed for a fixed number of data points n=100: the results are shown in Figure \ref{fig:RIDGE_MSE_Lambda}.
Small values of $\lambda$ (e.g., 0.001, 0.01) behave similarly to OLS, with the MSE decreasing as polynomial degree increases. However, as $\lambda$ increases (e.g., 1, 10, 100), the MSE curve flattens and even increases for high polynomial degrees, indicating that strong regularization prevents overfitting but may lead to underfitting if too large.
Among the tested values, $\lambda$ =0.01 produces the lowest MSE for most polynomial degrees, suggesting an optimal trade-off between bias and variance.

\input{Graphs/RIDGE_MSE_Lambda.tex}

Next, $\lambda = 0.01$ is selected to plot the MSE as a function of polynomial degree for different numbers of data points, allowing a direct comparison with the standard OLS regression, as shown in Figure \ref{fig:RIDGE_MSE_data}. 
The comparison reveals that while both methods achieve low MSE at moderate polynomial degrees, OLS exhibits a sharp increase in MSE at high degrees due to overfitting. In contrast, Ridge regression maintains a lower and more stable MSE across all degrees, demonstrating its effectiveness in controlling model complexity through regularization. This highlights Ridge's advantage in scenarios where overfitting is a concern, particularly with high-degree polynomials.
As one can see, the mse decreases as the polynomial degree increases, similar to OLS.
\input{Graphs/RIDGE_MSE_data.tex}

The parameters $\theta$ as a function of polynomial degree for Ridge regression are plotted. Compared to OLS, the coefficients remain more stable even at high polynomial degrees. 
This stability indicates that Ridge's $L_2$ regularization effectively constrains the magnitude of the coefficients, preventing them from becoming excessively large and reducing overfitting.
The result is a more robust model that generalizes better to unseen data.

\subsection{Gradient Descent Methods}
\label{Gradient Descent Methods}

The analytical expression for paramters $\theta$ is now compared to the results obtained using gradient descent methods.\\\\
First, the standard gradient descent method is applied to both OLS and Ridge regression, exploring the impact of different learning rates $\eta$. 
The results for OLS and Ridge with varying learning rates are shown in the figures below and it is evident that the gradient descent is very sensitive to the choice of the learning rate.\\
For example, with a learning rate equale to 0.01, as shown in the Figure \ref{fig:GD_OLS_LR}, the method converges to the analytical solution for polynomial degrees up to 15, with the MSE closely matching the analytical results.
For smaller learning rates, the algorithm does not reach the analytical solution within the set number of iterations, resulting in a higher MSE. 
For learning rate bigger than 0.01, the algorithm diverges, failing to converge to the analytical solution and causing the MSE to increase dramatically.
\input{Graphs/GD_OLS_LR.tex}
Similarly, for Ridge regression with $\lambda = 0.01$, as shown in the Figure \ref{fig:GD_RIDGE_LR}, a learning rate of 0.01 allows convergence to the analytical solution, while smaller rates lead to slower convergence and larger MSE.
\input{Graphs/GD_RIDGE_LR.tex}
 
In both cases, the analytical solution is more accurate than gradient descent, except for a learning rate of 0.01, where the two methods yield nearly identical results.\\\\
Next, advanced gradient descent methods are applied to both OLS and Ridge regression.
For OLS, as shown in the Figure \ref{fig:AGD_OLS}, Momentum, Adagrad, RMSprop, and Adam all successfully converge to the analytical solution with MSE values closely matching the analytical results across polynomial degrees.
 RMSprop and Adam show better performance at higher polynomial degrees, because of their adaptive learning rate mechanisms.

\input{Graphs/AGD_OLS.tex}

For Ridge regression with $\lambda = 0.01$, as shown in the Figure \ref{fig:AGD_RIDGE}, RMSProp and Adam show a better convergence at higher degrees, but still the analytical solution is the best one. 
\textbf{why analytical is better than advanced methods?}

\input{Graphs/AGD_RIDGE.tex}
\subsection{Lasso regression}
\label{Lasso regression}
Lasso regression does not have an analytical solution, so only gradient descent methods are applied.\\\\
First, the standard gradient descent method is applied to Lasso regression, exploring the impact of different learning rates $\eta$. 
The results for Lasso with varying learning rates are shown in the Figure \ref{fig:GD_LASSO_LR}.
With a learning rate equal to 0.01, the method converges for polynomial degrees up to 15, with the MSE being relatively low.
For smaller learning rates, the algorithm converges very slowly, requiring a large number of iterations to approach the minimum.
\input{Graphs/GD_LASSO_LR.tex}

When using more advanced gradient descent methods for Lasso regression, as shown in the Figure \ref{fig:AGD_LASSO}, the convergence improves significantly.
Ridge and Lasso regression show similar MSE across polynomial degrees.
However, the MSE values for Lasso are generally higher than those for Ridge regression, due to the nature of the $L_1$ penalty. 
This encourages many coefficients to become exactly zero, which makes the model more interpretable but increases the bias of the coefficients.
On the other hand, Ridge regression shrinks coefficients towards zero but does not set them exactly to zero, leading to lower bias and lower MSE.
Compared to OLS, which is unbiased but can have high variance, Lasso achieves a lower MSE by reducing the variance, even with the bias it introduces
\input{Graphs/AGD_LASSO.tex}

\subsection{Stochastic Gradient Descent}
\label{Stochastic Gradient Descent}
Stochastic Gradient Descent (SGD) is now applied to OLS, Ridge, and Lasso regression.
For OLS, as shown in Figure \ref{fig:AGD_OLS_SGD}, SGD converges to a solution with MSE values that are slightly higher than those obtained with standard gradient descent methods. 
This difference arises because SGD computes the gradient using individual samples or small mini-batches instead of the entire dataset. While this makes each update faster and more memory-efficient, it also introduces noise into the updates, leading to a less stable convergence.

\input{Graphs/AGD_OLS SGD.tex}

Ridge regression with SGD, as shown in Figure \ref{fig:AGD_RIDGE_SGD}, exhibits a similar pattern: the MSE is slightly higher compared to batch methods due to the stochastic nature of the updates. However, the $L_2$ penalty contributes to more stable coefficient shrinkage, so the convergence remains reliable and generalization can even improve compared to OLS.

\input{Graphs/AGD_RIDGE SGD.tex}

In the case of Lasso regression with SGD, shown in Figure \ref{fig:AGD_LASSO_SGD}, the optimization becomes more challenging because of the $L_1$ penalty, which creates flat regions and non-differentiable points at zero. Here, the stochasticity can both complicate and help: while it may cause small fluctuations and slightly higher MSE, it also allows the algorithm to escape plateaus and drive coefficients exactly to zero, reinforcing sparsity in the solution.

\input{Graphs/AGD_LASSO SGD.tex}

\subsection{Bias-Variance trade off}
\label{Bias-Variance trade off}
To illustrate the bias-variance trade-off, the Runge function is fitted using an OLS model with varying polynomial degrees. 
The bias, variance, and mean squared error (MSE) are computed using bootstrap resampling with 50 resamples. The results are shown in Figure \ref{fig:Bias_Var}.
For low polynomial degree the variance, bias and mse remain close to 0, indicating that the model is simple but stable.
As the degree of the polynomial increases at a certain point the variance jumps up dramatically, causing a corresponding rise in MSE.
This reflects overfitting, where the model starts capturing noise in the training data rather than just the underlying signal.\\\\
The optimal model complexity lies between these extremes, balancing bias and variance to minimize prediction error.
The bootstrap analysis highlights how the number of data points affects this balance: when the number of data points is low, the variance increases more rapidly with polynomial degree, leading to higher MSE, whil with more data points, the variance is reduced and the MSE stabiliez.\\
\input{Graphs/Bias_Var.tex}

\subsection{Cross Validation}
\label{Cross Validation}
Cross-validation is now applied to evaluate the performance of OLS, Ridge, and Lasso regression models.
The MSE is computed using 5-fold cross-validation for polynomial degrees ranging from 1 to 15, as shown in Figure \ref{fig:Cross_Validation}.
For OLS, the MSE decreases initially with increasing polynomial degree, but then rises sharply at higher degrees due to overfitting, especially with fewer data points.
Ridge regression, with a regularization parameter $\lambda = 0.01$, shows a more stable MSE across polynomial degrees, effectively controlling overfitting.
Lasso regression, also with $\lambda = 0.01$, exhibits a similar trend, but with slightly higher MSE than Ridge due to the stronger bias introduced by the $L_1$ penalty.       

These results highlight the bias-variance trade-off: while OLS achieves very low bias at high degrees, its variance becomes large.
Ridge and Lasso introduce a small bias but yield more stable models that generalize better.
\input{Graphs/Cross_Validation.tex}

\section{Discussion and Conclusion}
\label{sec:Discussion_and_Conclusion}
In this project we analyzed regression methods for fitting the Runge function, focusing on OLS, Ridge, and Lasso regression, and compared them using mean squared error (MSE) as the main performance measure.\\\\

OLS regression reproduced the function well at low polynomial degrees, but quickly overfitted when the degree increased, as reflected in strongly rising MSE.
Ridge regression introduced $L_2$ regularization, which stabilized the coefficients and reduced variance, achieving more reliable predictions across polynomial degrees.
Lasso regression, based on an $L_1$ penalty, further encouraged sparsity by setting coefficients exactly to zero.
This simplified the model but generally produced slightly higher MSE compared to Ridge due to the stronger bias introduced.\\\\

A central part of the project was the implementation and study of gradient descent methods.
Standard gradient descent for OLS and Ridge regression showed a strong dependence on the learning rate: too small rates resulted in slow convergence and higher MSE, while too large rates caused divergence.
With a carefully chosen learning rate, the results closely matched the analytical solutions.
Advanced optimizers such as momentum, Adagrad, RMSprop, and Adam significantly improved stability and convergence.
RMSprop and Adam in particular adapted learning rates during training, achieving consistent performance across polynomial degrees and bringing MSE close to the closed-form benchmarks.\\
For Lasso regression, which lacks an analytical solution, gradient-based optimization was essential.
Here, adaptive optimizers again provided faster and more stable convergence compared to plain gradient descent.
Stochastic gradient descent further reduced computation time by using individual samples, at the cost of noisier updates, but still produced results consistent with batch methods.\\\\

\textbf{Now some parts about the bootsrtap, croos validation and bias variance trade off}


\bibliographystyle{apsrev4-2}
\bibliography{References} % Your .bib file


\end{document}
